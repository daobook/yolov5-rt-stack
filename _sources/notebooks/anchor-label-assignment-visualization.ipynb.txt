{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可视化 anchor-target 赋值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deterministic REPRODUCIBILITY\n",
    "torch.manual_seed(24)\n",
    "random.seed(5)\n",
    "np.random.seed(95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import contextlib\n",
    "import cv2\n",
    "from torchvision.ops import box_convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolort.data.data_module import COCODetectionDataModule\n",
    "from yolort.models.transform import YOLOTransform\n",
    "from yolort.utils.image_utils import (\n",
    "    color_list,\n",
    "    plot_one_box,\n",
    "    cv2_imshow,\n",
    "    load_names,\n",
    "    parse_single_image,\n",
    "    parse_images,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为测试设定 coco128 dataset 和 dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get COCO label names and COLORS list\n",
    "LABELS = (\n",
    "    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train',\n",
    "    'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
    "    'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
    "    'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon',\n",
    "    'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot',\n",
    "    'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant',\n",
    "    'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
    "    'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n",
    "    'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
    "    'hair drier', 'toothbrush',\n",
    ")\n",
    "\n",
    "COLORS = color_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 455147521 (char 455147520)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/media/pc/data/4tb/lxw/books/yolov5-rt-stack/docs/source/notebooks/anchor-label-assignment-visualization.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bxin/media/pc/data/4tb/lxw/books/yolov5-rt-stack/docs/source/notebooks/anchor-label-assignment-visualization.ipynb#ch0000007vscode-remote?line=7'>8</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m8\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxin/media/pc/data/4tb/lxw/books/yolov5-rt-stack/docs/source/notebooks/anchor-label-assignment-visualization.ipynb#ch0000007vscode-remote?line=9'>10</a>\u001b[0m \u001b[39mwith\u001b[39;00m contextlib\u001b[39m.\u001b[39mredirect_stdout(io\u001b[39m.\u001b[39mStringIO()):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bxin/media/pc/data/4tb/lxw/books/yolov5-rt-stack/docs/source/notebooks/anchor-label-assignment-visualization.ipynb#ch0000007vscode-remote?line=10'>11</a>\u001b[0m     datamodule \u001b[39m=\u001b[39m COCODetectionDataModule(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxin/media/pc/data/4tb/lxw/books/yolov5-rt-stack/docs/source/notebooks/anchor-label-assignment-visualization.ipynb#ch0000007vscode-remote?line=11'>12</a>\u001b[0m         image_root,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxin/media/pc/data/4tb/lxw/books/yolov5-rt-stack/docs/source/notebooks/anchor-label-assignment-visualization.ipynb#ch0000007vscode-remote?line=12'>13</a>\u001b[0m         anno_path\u001b[39m=\u001b[39;49mannotation_path,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxin/media/pc/data/4tb/lxw/books/yolov5-rt-stack/docs/source/notebooks/anchor-label-assignment-visualization.ipynb#ch0000007vscode-remote?line=13'>14</a>\u001b[0m         skip_val_set\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxin/media/pc/data/4tb/lxw/books/yolov5-rt-stack/docs/source/notebooks/anchor-label-assignment-visualization.ipynb#ch0000007vscode-remote?line=14'>15</a>\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxin/media/pc/data/4tb/lxw/books/yolov5-rt-stack/docs/source/notebooks/anchor-label-assignment-visualization.ipynb#ch0000007vscode-remote?line=15'>16</a>\u001b[0m     )\n",
      "File \u001b[0;32m/media/pc/data/4tb/lxw/anaconda3/envs/torch/lib/python3.10/site-packages/yolort/data/data_module.py:111\u001b[0m, in \u001b[0;36mCOCODetectionDataModule.__init__\u001b[0;34m(self, data_path, anno_path, num_classes, data_task, train_set, val_set, skip_train_set, skip_val_set, train_transform, val_transform, batch_size, num_workers, *args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m train_ann_file \u001b[39m=\u001b[39m anno_path \u001b[39m/\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdata_task\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mtrain_set\u001b[39m}\u001b[39;00m\u001b[39m.json\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m val_ann_file \u001b[39m=\u001b[39m anno_path \u001b[39m/\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdata_task\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mval_set\u001b[39m}\u001b[39;00m\u001b[39m.json\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m train_dataset \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 111\u001b[0m     \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m skip_train_set \u001b[39melse\u001b[39;00m COCODetection(data_path, train_ann_file, train_transform())\n\u001b[1;32m    112\u001b[0m )\n\u001b[1;32m    113\u001b[0m val_dataset \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m skip_val_set \u001b[39melse\u001b[39;00m COCODetection(data_path, val_ann_file, val_transform())\n\u001b[1;32m    115\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[1;32m    116\u001b[0m     train_dataset\u001b[39m=\u001b[39mtrain_dataset,\n\u001b[1;32m    117\u001b[0m     val_dataset\u001b[39m=\u001b[39mval_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    122\u001b[0m )\n",
      "File \u001b[0;32m/media/pc/data/4tb/lxw/anaconda3/envs/torch/lib/python3.10/site-packages/yolort/data/coco.py:16\u001b[0m, in \u001b[0;36mCOCODetection.__init__\u001b[0;34m(self, img_folder, ann_file, transforms, return_masks)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, img_folder, ann_file, transforms, return_masks\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m---> 16\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(img_folder, ann_file)\n\u001b[1;32m     17\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transforms \u001b[39m=\u001b[39m transforms\n\u001b[1;32m     19\u001b[0m     json_category_id_to_contiguous_id \u001b[39m=\u001b[39m {v: i \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoco\u001b[39m.\u001b[39mgetCatIds())}\n",
      "File \u001b[0;32m/media/pc/data/4tb/lxw/anaconda3/envs/torch/lib/python3.10/site-packages/torchvision/datasets/coco.py:36\u001b[0m, in \u001b[0;36mCocoDetection.__init__\u001b[0;34m(self, root, annFile, transform, target_transform, transforms)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(root, transforms, transform, target_transform)\n\u001b[1;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpycocotools\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcoco\u001b[39;00m \u001b[39mimport\u001b[39;00m COCO\n\u001b[0;32m---> 36\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoco \u001b[39m=\u001b[39m COCO(annFile)\n\u001b[1;32m     37\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mids \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39msorted\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoco\u001b[39m.\u001b[39mimgs\u001b[39m.\u001b[39mkeys()))\n",
      "File \u001b[0;32m/media/pc/data/4tb/lxw/anaconda3/envs/torch/lib/python3.10/site-packages/pycocotools-2.0.4-py3.10-linux-x86_64.egg/pycocotools/coco.py:82\u001b[0m, in \u001b[0;36mCOCO.__init__\u001b[0;34m(self, annotation_file)\u001b[0m\n\u001b[1;32m     80\u001b[0m tic \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     81\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(annotation_file, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> 82\u001b[0m     dataset \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mload(f)\n\u001b[1;32m     83\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mtype\u001b[39m(dataset)\u001b[39m==\u001b[39m\u001b[39mdict\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mannotation file format \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m not supported\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mtype\u001b[39m(dataset))\n\u001b[1;32m     84\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mDone (t=\u001b[39m\u001b[39m{:0.2f}\u001b[39;00m\u001b[39ms)\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(time\u001b[39m.\u001b[39mtime()\u001b[39m-\u001b[39m tic))\n",
      "File \u001b[0;32m/media/pc/data/4tb/lxw/anaconda3/envs/torch/lib/python3.10/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(fp, \u001b[39m*\u001b[39m, \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_float\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_constant\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_pairs_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m     \u001b[39m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[39m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[39mreturn\u001b[39;00m loads(fp\u001b[39m.\u001b[39;49mread(),\n\u001b[1;32m    294\u001b[0m         \u001b[39mcls\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mcls\u001b[39;49m, object_hook\u001b[39m=\u001b[39;49mobject_hook,\n\u001b[1;32m    295\u001b[0m         parse_float\u001b[39m=\u001b[39;49mparse_float, parse_int\u001b[39m=\u001b[39;49mparse_int,\n\u001b[1;32m    296\u001b[0m         parse_constant\u001b[39m=\u001b[39;49mparse_constant, object_pairs_hook\u001b[39m=\u001b[39;49mobject_pairs_hook, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m/media/pc/data/4tb/lxw/anaconda3/envs/torch/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/media/pc/data/4tb/lxw/anaconda3/envs/torch/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m     \u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[1;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/media/pc/data/4tb/lxw/anaconda3/envs/torch/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 455147521 (char 455147520)"
     ]
    }
   ],
   "source": [
    "# Acquire the images and labels from the coco128 dataset\n",
    "data_path = Path('/media/pc/data/4tb/lxw/tests/datasets')\n",
    "coco128_dirname = 'coco'\n",
    "coco128_path = data_path / coco128_dirname\n",
    "image_root = coco128_path / 'images' / 'train2017'\n",
    "annotation_path = coco128_path / 'annotations'\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "with contextlib.redirect_stdout(io.StringIO()):\n",
    "    datamodule = COCODetectionDataModule(\n",
    "        image_root,\n",
    "        anno_path=annotation_path,\n",
    "        skip_val_set=True,\n",
    "        batch_size=batch_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = iter(datamodule.train_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 采样图片和目标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, annotations = next(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = random.randrange(batch_size)\n",
    "img_raw = cv2.cvtColor(parse_single_image(images[idx]), cv2.COLOR_RGB2BGR)  # For visualization\n",
    "\n",
    "for box, label in zip(annotations[idx]['boxes'].tolist(), annotations[idx]['labels'].tolist()):\n",
    "    img_raw = plot_one_box(box, img_raw, color=COLORS[label % len(COLORS)], label=LABELS[label])\n",
    "\n",
    "cv2_imshow(img_raw, imshow_scale=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Batch in Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolort.models import yolov5s\n",
    "\n",
    "model = yolov5s()\n",
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, targets = model.transform(images, annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = parse_images(samples.tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attach_idx = torch.where(targets[:, 0].to(dtype=torch.int32) == idx)[0]\n",
    "\n",
    "img_training = cv2.cvtColor(inputs[idx], cv2.COLOR_RGB2BGR)  # For visualization\n",
    "img_h, img_w = img_training.shape[:2]\n",
    "targets_training = targets[attach_idx]\n",
    "for box, label in zip(targets_training[:, 2:], targets[attach_idx][:, 1]):\n",
    "    label = int(label.tolist())\n",
    "    box = box_convert(box, in_fmt='cxcywh', out_fmt='xyxy')\n",
    "    box = (box * torch.tensor([img_h, img_w, img_h, img_w])).tolist()\n",
    "    img_training = plot_one_box(box, img_training, color=COLORS[label % len(COLORS)], label=LABELS[label])\n",
    "\n",
    "cv2_imshow(img_training, imshow_scale=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提取中间特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolort.utils import FeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_features = FeatureExtractor(model.model, return_layers=['backbone', 'head'])\n",
    "\n",
    "intermediate_features = yolo_features(samples.tensors, targets)\n",
    "\n",
    "features = intermediate_features['backbone']\n",
    "head_outputs = intermediate_features['head']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取 Anchors 和 Strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = len(head_outputs)\n",
    "\n",
    "anchors = torch.as_tensor(model.model.anchor_generator.anchor_grids, dtype=torch.float32, device=device)\n",
    "strides = torch.as_tensor(model.model.anchor_generator.strides, dtype=torch.float32, device=device)\n",
    "anchors = anchors.view(num_layers, -1, 2) / strides.view(-1, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign Targets to Anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build targets for compute_loss(), input targets(image,class,x,y,w,h)\n",
    "num_anchors = len(model.model.anchor_generator.anchor_grids)  # number of anchors\n",
    "num_targets = len(targets)  # number of targets\n",
    "\n",
    "targets_cls, targets_box, anchors_encode = [], [], []\n",
    "indices = []\n",
    "grid_assigner = []  # Anchor Visulization\n",
    "gain = torch.ones(7, device=device)  # normalized to gridspace gain\n",
    "# same as .repeat_interleave(num_targets)\n",
    "ai = torch.arange(num_anchors, device=device).float().view(num_anchors, 1).repeat(1, num_targets)\n",
    "targets_append = torch.cat((targets.repeat(num_anchors, 1, 1), ai[:, :, None]), 2)  # append anchor indices\n",
    "\n",
    "g = 0.5  # bias\n",
    "off = torch.tensor([[0, 0],\n",
    "                    [1, 0], [0, 1], [-1, 0], [0, -1],  # j,k,l,m\n",
    "                    # [1, 1], [1, -1], [-1, 1], [-1, -1],  # jk,jm,lk,lm\n",
    "                    ], device=device).float() * g  # offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_threshold = 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's actually going on is the image is subdivided into a grid of squares, and the coordinates in `grid[]` are the coordinates of the upper-left corner of that square."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network provides $x$, $y$ coordinates in the range $(0, 1)$ (enforced by sigmoid) which covers the square, centered at 0.5. Multiplying by two allows detected $x$, $y$ coordinates to cover a larger range, slightly outside the square -- otherwise it's difficult to detect objects centered at grid boundaries. Subtracting 0.5 shifts the resulting range to $(-0.5, 1.5)$ which is centered around $(0, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_layers):\n",
    "    anchors_per_layer = anchors[i]\n",
    "    gain[2:6] = torch.tensor(head_outputs[i].shape)[[3, 2, 3, 2]]  # xyxy gain\n",
    "\n",
    "    # Match targets to anchors\n",
    "    targets_with_gain = targets_append * gain\n",
    "    if num_targets:\n",
    "        # Matches\n",
    "        ratios_wh = targets_with_gain[:, :, 4:6] / anchors_per_layer[:, None]  # wh ratio\n",
    "        ratios_filtering = torch.max(ratios_wh, 1. / ratios_wh).max(2)[0]\n",
    "        inds = torch.where(ratios_filtering < anchor_threshold)\n",
    "        targets_with_gain = targets_with_gain[inds]  # filter\n",
    "\n",
    "        # Offsets\n",
    "        grid_xy = targets_with_gain[:, 2:4]  # grid xy\n",
    "        grid_xy_inverse = gain[[2, 3]] - grid_xy  # inverse\n",
    "        inds_jk = (grid_xy % 1. < g) & (grid_xy > 1.)\n",
    "        inds_lm = (grid_xy_inverse % 1. < g) & (grid_xy_inverse > 1.)\n",
    "        inds_ones = torch.ones_like(inds_jk[:, 0])[:, None]\n",
    "        inds = torch.cat((inds_ones, inds_jk, inds_lm), dim=1).T\n",
    "        targets_with_gain = targets_with_gain.repeat((5, 1, 1))[inds]\n",
    "        offsets = (torch.zeros_like(grid_xy)[None] + off[:, None])[inds]\n",
    "    else:\n",
    "        targets_with_gain = targets_append[0]\n",
    "        offsets = torch.tensor(0, device=device)\n",
    "\n",
    "    # Define\n",
    "    bc = targets_with_gain[:, :2].long().T  # image, class\n",
    "    grid_xy = targets_with_gain[:, 2:4]  # grid xy\n",
    "    grid_wh = targets_with_gain[:, 4:6]  # grid wh\n",
    "    grid_ij = (grid_xy - offsets).long()\n",
    "\n",
    "    # Append\n",
    "    a = targets_with_gain[:, 6].long()  # anchor indices\n",
    "    # image, anchor, grid indices\n",
    "    indices.append((bc[0], a, grid_ij[:, 1].clamp_(0, gain[3] - 1),\n",
    "                    grid_ij[:, 0].clamp_(0, gain[2] - 1)))\n",
    "    targets_box.append(torch.cat((grid_xy - grid_ij, grid_wh), 1))  # box\n",
    "    grid_assigner.append(torch.cat((grid_xy, grid_wh), 1))\n",
    "    anchors_encode.append(anchors_per_layer[a])  # anchors\n",
    "    targets_cls.append(bc[1])  # class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visulization Anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolort.utils.image_utils import anchor_match_visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_with_anchor = anchor_match_visualize(samples.tensors, grid_assigner, indices, anchors_encode, head_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2_imshow(images_with_anchor[idx], imshow_scale=0.5)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "YOLOv5 Tutorial",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('torch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "20e538bd0bbffa4ce75068aaf85df10d4944f3fdb705eeec6781a4702773116f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
